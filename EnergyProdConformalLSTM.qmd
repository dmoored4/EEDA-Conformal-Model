---
title: "Conformalizing an LSTM to Optimize Revenue for a Renewable Energy Operator using Conditional Value at Risk"
subtitle: Energy Markets and Data Analytics | Rutgers, Spring 2024 | Dr. Robert Mieth
author: "Daniel Moore, Laila Saleh"
date: "May 8, 2024"
abstract: "We optimized the market trading of a renewable energy generation operator with conditional value at risk based on probabilistic forecasts made with a conformalized Long Short-term Memory (LSTM) recurrent neural network. This work demonstrates an end-to-end workflow of how field data can be ingested, analyzed, and exploited to reduce risk exposure for the operator. This is financially beneficial to the individual operator, but taken to a large scale this methodology increases the incentive for renewable generation participation which should drive cost and emissions down. This work used only eight features with favorable results, so it is expected that further studies and more advanced models with the same architecture would provide better yield."

format:
  html:
    toc: true
    number-depth: 2
    number-sections: true
    start_page_number: 3
    
    lot: true
    tbl-cap-location: top

    lof: true
    fig-align: center
    fig-cap-location: bottom
    
execute: 
  echo: FALSE

engine: julia
---

\newpage

```{julia}
#| label: loading-packages
#| output: false
using Dates
using CSV, DataFrames, StatsPlots, StatsBase
using Flux

gr(
    # setting default font
    fontfamily=:Times,
    # outputting as Scalar Vector Graphic (SVG) for nice plots to go to the PDF.
    fmt=:svg
)
```

# Introduction

The [IEEE Hybrid Energy Forecasting and Trading Competition](https://ieee-dataport.org/competitions/hybrid-energy-forecasting-and-trading-competition) challenges participants to make day-ahead, half-hourly probabilistic forecasts of solar and wind energy production for a solar farm and Hornsea-1 Wind Farm in the east of England with a combined 3.6 GW capacity. The second task is to decide how much energy to commit to selling at the day-ahead price (DAP) to optimize revenues. Any difference between the committed energy and actual energy is rewarded or punished according to the single settlement price (SSP). As discussed later in the data, beyond their volatility, the DAP and SSP can also be negative indicating a surplus of energy on the market. In rare events, underproduction could be rewarded due to a negative SSP. The implied task is to also forecast the market prices so that the operator can reduce their risk exposure from both the energy production and market prices.

## Motivation

This is an appropriate capstone project for this course as it applies many topics covered ranging from unit commitment and energy market trading to advanced predictive and prescriptive analytics for complex and uncertain events. It is an interesting and practical opportunity to wrestle with the available resources to make the best decisions for the operator. Lastly, we find it a compelling problem because reducing the risk for renewable energy generation operators will encourage more participation and be of a net benefit to investors, consumers, and the environment - a rare triple-win.

## Objectives

We will show an end-to-end workflow where we process data to train a forecasting model and conformalize it so that its point forecasts can be transformed into probabilistic forecasts. These forecasts enable us to make market-trading decisions that consider the uncertainty in not only energy production but also in the market itself. Finally, we will demonstrate the financial benefit of leveraging the power of stochastic optimization to reduce the risk exposure of the operator.

## Literature Review

What have other people done

# Data Analysis

We have obtained datasets from two sources: the competition itself which provides the energy production data through the [Rebase API](https://www.rebase.energy/challenges/heftcom2024) and the [VisualCrossing API](https://www.visualcrossing.com/weather-api) which provides the weather data. The energy data details the solar and wind energy production and the DAP and SSP in half-hourly increments. The weather data is treated as historic for the period preceding a given forecast and as a weather forecast for the forecast horizon. If deployed, the model would need to operate only using forecasted weather data. This approach is acceptable for this study as 24-hour-ahead weather forecasts are typically very accurate and we are only incorporating basic weather features.

## Data Summary

The tables below provide a sample of what the data from each source look like for a few observation times and summary statistics.

```{julia}
#| label: loading-data
#| output: false
energy_cols = [:Solar, :Wind, :TotalEnergy, :DAP, :SSP]
weather_cols = [:temp, :windspeed, :winddir, :cloudcover, :visibility]

function getdata()
    data = CSV.read("src/data/data_rebase.csv", DataFrame)

    select!(data,
        :timestamp_utc => :DateTime,
        :solar_act => :Solar,
        :wind_act => :Wind,
        :dayahead_price => :DAP,
        :imbalance_price => :SSP)

    data.TotalEnergy = data.Solar + data.Wind


    data.temptime = floor.(data.DateTime, Hour(1))


    unique!(data)

    weather = CSV.read("src/data/hornsea 2024-02-29 to 2024-04-06.csv", DataFrame)
    weather = [weather; CSV.read("src/data/hornsea 2024-04-06 to 2024-04-30.csv", DataFrame)]

    select!(weather,
        :datetime => :DateTime,
        :temp,
        :windspeed,
        :winddir,
        :cloudcover,
        :visibility
    )

    unique!(weather)

    data = leftjoin(data, weather, on=:temptime => :DateTime)

    filter!(row -> !any(ismissing, row), data)

    select!(data, Not(:temptime))
    select!(data, [:DateTime; weather_cols; energy_cols])

    data[!, Not(:DateTime)] = Float32.(data[!, Not(:DateTime)])

    sort!(data, :DateTime)

    return data
end

data = getdata()
```

```{julia}
#| label: rebase-data
#| tbl-cap: "Data from RebaseAPI"
data[201:205, [:DateTime; energy_cols]]
```

```{julia}
#| label: rebase-data-summary
#| tbl-cap: "Energy Data Summary"
describe(data, :mean, :min, :median, :max, cols=energy_cols)
```

```{julia}
#| label: weather-data
#| tbl-cap: "Data from VisualCrossing"
data[201:205, [:DateTime; weather_cols]]
```

```{julia}
#| label: weather-data-summary
#| tbl-cap: "Weather Data Summary"
describe(data, :mean, :min, :median, :max, cols=[:DateTime; weather_cols])
```

## Data Visualizations

```{julia}
#| label: plotting-conveniences
#| output: false
function every4hrs_date_noon(dt)
    if hour(dt) % 4 == 0 & minute(dt) == 0
        if hour(dt) == 12
            Dates.format(dt, "u-d HH:MM")
        else
            Dates.format(dt, "HH:MM")
        end
    end
end


monthly_ticks = (
    DateTime(Date(data.DateTime[findfirst(d -> dayofweek(d) == Sun, data.DateTime)]), Time(12)):Week(1):last(data.DateTime), Dates.format.(
        DateTime(Date(data.DateTime[findfirst(d -> dayofweek(d) == Sun, data.DateTime)]), Time(12)):Week(1):last(data.DateTime),
        "u-d"
    )
)

daily_xticks = (Time(0):Hour(3):Time(23), Dates.format.(Time(0):Hour(3):Time(23), "HH:MM"))

cmap = Dict(
    "DAP" => :darkgreen,
    "SSP" => :orange,
    "temp" => :darkred,
    "Solar" => :darkgoldenrod1,
    "Wind" => :darkblue,
    "CloudCover" => :darkgrey,
    "Total Energy" => :hotpink,
)

clouds = cgrad([:gold, :darkgrey])
winds = cgrad([:white, :blue])
temp = cgrad(:turbo)
visibiilty = cgrad([:white, :darkblue])

april_week_3 = DateTime(2024, 4, 14, 8, 30) .≤ data.DateTime .≤ DateTime(2024, 4, 21, 8, 00)
```

The first thing to notice when looking at the DAP and SSP time-series plots below is the volatility of the prices. The DAP exhibits some seasonality but the trend and cycles are not entirely obvious. The SSP is more volatile as it traverses from the daily minimum to the daily maximum several times in a given day. This highlights how difficult it is for all participants to make accurate forecasts and fulfill their commitments. The plots of the third week of April provide a closer look at the characteristics of these prices. While predicting the DAP appears feasible, the SSP is hardly distinguishable from noise.

```{julia}
#| label: monthly-energy-price-plot
#| fig-cap: "March & April 2024 Energy Prices"
#| fig-alt: "Plot of day-ahead and single settlement prices for March & April 2024"
@df data plot(
    titlefontsize=8,
    guidefontsize=8,
    xtickfontsize=5,
    legend=false,
    ylims=1.1 .* extrema(:SSP),
    begin
        plot(
            title="March & April 2024",
            ylabel="DAP",
            xticks=monthly_ticks,
            :DateTime, :DAP, color=cmap["DAP"]
        )

        hline!([0], lw=0.5, lc=:black, α=0.5, label=false)
    end,
    begin
        plot(
            title="Week 3, April 2024",
            :DateTime[april_week_3], :DAP[april_week_3], color=cmap["DAP"]
        )

        hline!([0], lw=0.5, lc=:black, α=0.5, label=false)
    end,
    begin
        plot(
            ylabel="SSP",
            xticks=monthly_ticks,
            :DateTime, :SSP, color=cmap["SSP"]
        )

        hline!([0], lw=0.5, lc=:black, α=0.5, label=false)
    end,
    begin
        plot(
            :DateTime[april_week_3], :SSP[april_week_3], color=cmap["SSP"]
        )

        hline!([0], lw=0.5, lc=:black, α=0.5, label=false)
    end
)
```

Figure @ref shows the energy generation over March 2024 while Table shows the first week of April 2024. We see solar energy production is, as expected, seasonal while wind energy production is very cyclical. It appears to go from nothing to its full capacity in a short amount of time and stay there for a random amount of time before dropping, usually back to nothing.

```{julia}
#| label: monthly-energy-production-plot
#| fig-cap: "March & April 2024 Energy Production"
#| fig-alt: "Plot of wind solar and total energy production for March & April 2024"
@df data plot(
    titlefontsize=8,
    guidefontsize=8,
    xtickfontsize=5,
    legend=false,
    plot(
        title="March & April 2024",
        ylabel="Solar",
        xticks=monthly_ticks,
        :DateTime, :Solar, color=cmap["Solar"]
    ),
    plot(
        title="Week 3, April 2024",
        :DateTime[april_week_3], :Solar[april_week_3], color=cmap["Solar"]
    ),
    plot(
            ylabel="Wind",
            xticks=monthly_ticks,
            :DateTime, :Wind, color=cmap["Wind"]
    ),
    plot(
        :DateTime[april_week_3], :Wind[april_week_3], color=cmap["Wind"]
    )
)
```

```{julia
#| label: price_vs_temperature
#| fig-cap: "DAP and SSP vs Temperature"
#| fig-alt: "Plot of DAP and SSP vs Temperature"

@df data plot(
    titlefontsize=8,
    guidefontsize=8,
    xtickfontsize=5,
    legend=false,
    ylims=1.1 .* extrema(:SSP),
    begin
        plot(
            title="March & April 2024",
            ylabel="DAP",
            xticks=daily_xticks,
            Time.(:DateTime), :DAP,
            group=Date.(:DateTime), line_z=:temp,
            color=temp
        )

        hline!([0], lw=0.5, lc=:black, α=0.5, label=false)
    end,
    begin
        plot(
            title="Week 3, April 2024",
            :DateTime[april_week_3], :DAP[april_week_3], color=cmap["DAP"]
        )

        hline!([0], lw=0.5, lc=:black, α=0.5, label=false)
    end,
    begin
        plot(
            ylabel="SSP",
            xticks=monthly_ticks,
            :DateTime, :SSP, color=cmap["SSP"]
        )

        hline!([0], lw=0.5, lc=:black, α=0.5, label=false)
    end,
    begin
        plot(
            :DateTime[april_week_3], :SSP[april_week_3], color=cmap["SSP"]
        )

        hline!([0], lw=0.5, lc=:black, α=0.5, label=false)
    end,
        begin
        plot(
            ylabel="SSP",
            xticks=monthly_ticks,
            :DateTime, :SSP, color=cmap["SSP"]
        )

        hline!([0], lw=0.5, lc=:black, α=0.5, label=false)
    end,
    begin
        plot(
            :DateTime[april_week_3], :SSP[april_week_3], color=cmap["SSP"]
        )

        hline!([0], lw=0.5, lc=:black, α=0.5, label=false)
    end
)
```

# Long Short-Term Memory Model

We used an LSTM Recurrent Neural Network (RNN) to predict the energy production and market prices for the following day. RNNs are designed to handle time-series data and LSTMs are a special type of RNN that can learn long-term dependencies in the data. Combined with a dense neural network, this model can remember (and forget) time-dependent relationships and approximate the complex dynamics among the variables. 

```{julia}
#| label: data-transforms
#| output: false
transforms = Dict(
    col_name =>
        fit(UnitRangeTransform, data[!, col_name]) for col_name in [weather_cols..., energy_cols...]
)


function transform_data(data;
    dt_col = :DateTime, weather_cols = weather_cols, energy_cols = energy_cols)
    
    data_xfrm = transform(select(data, [dt_col, weather_cols..., energy_cols...]),
        renamecols=false,
        dt_col =>
            dt -> -(1/2) * (1 .+ cos.((hour.(dt) + minute.(dt)/60) * 24/(2π))),
        [col => x -> StatsBase.transform(transforms[col], x) for col in vcat(weather_cols, energy_cols)]
    )

    return data_xfrm
end

function reconstruct_data!(data;
    weather_cols = [], energy_cols = energy_cols)
    data_xfrm = transform(
        select(data, [weather_cols..., energy_cols...]),
        renamecols=false,
        [col => x -> StatsBase.reconstruct!(transforms[col], x) for col in vcat(weather_cols, energy_cols)]
    )

    return data_xfrm
end

function batchsequences(data; batch_size = 2^5)
    M = Matrix(data)' .|> Float32

    # first turn into a vector of n x batches
    # n is number of features
    batches = [M[:, i:i+batch_size-1] for i in 1:size(M, 2)-batch_size+1]
    
    #=
    now we need to offset everything a couple different ways. we need to have the inputs for predicting the energy at time, t be:
    - the time and weather at time t
    - the energy at time t-1

    Then we need the target to be the energy at time t
    =# 
    next_time_weather = [b[1:1+length(weather_cols), :] for b in batches[2:end]]
    
    this_energy = [b[end-length(energy_cols)+1:end, :] for b in batches[1:end-1]]
    
    past = [[w; e] for (w, e) in zip(next_time_weather, this_energy)]

    next_energy = this_energy[2:end]

    batches = [(past=p, next=n) for (p, n) in zip(past, next_energy)]

    return batches
end
```

```{julia}
#| label: data-split
#| output: false
train_ratio = 0.45
calib_ratio = 0.35

train_idx = 1:floor(Int, train_ratio * nrow(data))
calib_idx = (last(train_idx) + 1):floor(Int, (train_ratio + calib_ratio) * nrow(data))
test_idx = (last(calib_idx) + 1):nrow(data)

Train = batchsequences(transform_data(data[train_idx, :]))
Calib = batchsequences(transform_data(data[calib_idx, :]))
Test = batchsequences(transform_data(data[test_idx, :]))
```

## Training
We used Julia's deep learning library, Flux, to build and train the LSTM. We created a multi-target regressor to predict the energy production and market prices in single timestep increments. All features were normalized and time was encoded as a sine wave to capture the cyclical nature of time.

```{julia}
#| label: set-loss
#| output: false
#| echo: true
#| eval: false
loss(model, x, y) = Flux.mse(model(x), y)
loss(model, data) = loss(model, data.past, data.next)
```

```{julia}
#| label: build-LSTM
#| echo: true
#| code-fold: show
#| eval: false
input_dims = 1 + length(weather_cols) + length(energy_cols)
out_dims = length(energy_cols)
hidden_dim = 2^3
model = Chain(
    LSTM_in=LSTM(input_dims, hidden_dim),
    LSTM_hidden=LSTM(hidden_dim, hidden_dim),
    Dense_out=Dense(hidden_dim, out_dims, σ)
)

train_loss_log = [loss(model, first(Train))]
test_loss_log = [loss(model, first(Test))]

model
```

```{julia}
#| label: setup-optimizer
#| echo: true
#| output: false
#| eval: false
η = 1e-4
opt_state = Flux.setup(Adam(η), model)
```

```{julia}
#| label: train-function
#| output: false
#| eval: false
function train!(loss, model, data, opt_state; loss_log=[])
    L, ∇ = Flux.withgradient(loss, model, data)

    if !isfinite(L)
        @warn "Loss value, \"$L\" is invalid."
    else
        Flux.update!(opt_state, model, ∇[1])
    end

    push!(loss_log, L)
end
```

```{julia}
#| label: training-loop
#| output: false
#| echo: true
#| eval: false

# Number of epochs
epochs = 2^8

for epoch in 1:epochs

    # intializing a log for the training loss for this epoch
    temp_train_log = []
    # Resetting the model state
    Flux.reset!(model)
    # conditioning on the first training sequence
    model(first(Train).past)
    # training on the first training sequence
    for T in Train[2:end]
        train!(loss, model, T, opt_state, loss_log=temp_train_log)
    end
    
    # logging the mean training loss for this epoch
    push!(train_loss_log, mean(temp_train_log))
    Flux.reset!(model)
    model(first(Test).past)
    push!(test_loss_log, mean(loss(model, T) for T in Test))
end
```

```{julia}
#| label: plot-loss
#| fig-cap: "Training and Testing Loss"
#| fig-alt: "Plot of training and testing loss"
#| eval: false
plot(
    title="Training and Testing Loss",
    xlabel="Epochs",
    ylabel="Mean Squared Error",
    [train_loss_log test_loss_log], label=["Training Loss" "Testing Loss"],
    markershape=[:o :^], markerstrokewidth=0,
    xscale=:log2, xticks= 2 .^ (0:8)
)
```

```{julia}
#| label: make_predictions
#| output: false
function predict_daterange(df, first_prediction, last_prediction)
    first_pred_index = findfirst(
        dt -> dt ≥ first_prediction, df.DateTime)
    last_pred_index = findlast(
        dt -> dt ≤ last_prediction, df.DateTime)

    pred_dates = df[first_pred_index:last_pred_index, :DateTime]

    df = transform_data(df)
    batched = batchsequences(df, batch_size=1)

    Flux.reset!(model)
    for i in 1:first_pred_index-1
        model(batched[i].past)
    end

    results = [model(batched[first_pred_index].past)]
    # iterate over the rest of the data
    # use the time and weather columns for batch[j][time+weather, ] and the last of predictions
    for j in first_pred_index+1:last_pred_index
        new_result = model([batched[j].past[1:1+length(weather_cols), :]; results[end]])
        push!(results, new_result)
    end

    results = hcat(results...)'

    results = DataFrame(results, energy_cols) |> reconstruct_data!

    results.DateTime = pred_dates
    
    return results
end

function predict_all(data)
    days = unique(Date.(data.DateTime))
    results = []
    for d in days[1:end-1]
        first_prediction = DateTime(d, Time(8, 30))
        last_prediction = first_prediction + Day(1)
        push!(results, predict_daterange(data, first_prediction, last_prediction))
    end

    results = vcat(results...)

    select!(results, [:DateTime; energy_cols])

    # rename all energy_cols to have "_pred" appended
    for col in energy_cols
        rename!(results, col => Symbol(string(col, "_pred")))
    end

    return results
end
```

```{julia}
#| label: one-day-predictions
#| eval: false
all_predictions = predict_all(data)

all_predictions = leftjoin(all_predictions, data, on=:DateTime)

CSV.write("src/data/predictions.csv", all_predictions)
```

## Performance

Is it any good?

# Conformalizing LSTM

What are conformal predictions? Why are they good

## How to?

How did we conformalize the LSTM?

## Performance

Is it any good?

# Conditional Value at Risk

What is CVAR

Why do we do it?

## Implementation

How did we do it?

## Performance

Was it any good?

# Conclusions

Was any of this worth while?

What did we learn?

What could others do?